{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgYCWDOhsZLV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1cfaed5f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import matplotlib\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "%matplotlib inline \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xob1nfIPxN7e",
    "outputId": "cca60067-54a8-4562-d24b-7689a1b0cb69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1aT120TILDKNsqMbV9w-APglATaMJ3lKY\n",
      "To: /content/webnlg-dataset-master.zip\n",
      "100% 12.4M/12.4M [00:00<00:00, 75.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1aT120TILDKNsqMbV9w-APglATaMJ3lKY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg4C80BXsyMS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqnnGfoUxdre"
   },
   "outputs": [],
   "source": [
    "!unzip /content/webnlg-dataset-master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcOZkJXySM3i",
    "outputId": "6256929f-0866-43d1-bcfb-bb0a46c1563b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['geeks', 'for', 'geeks']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "  \n",
    "def camel_case_split(strs):\n",
    "    words = [[strs[0]]]\n",
    "  \n",
    "    for c in strs[1:]:\n",
    "        if words[-1][-1].islower() and c.isupper():\n",
    "            words.append(list(c))\n",
    "        else:\n",
    "            words[-1].append(c)\n",
    "  \n",
    "    return \" \".join([''.join(word) for word in words])\n",
    "def replace_(strs):\n",
    "\n",
    "  return strs.replace('_',\" \")\n",
    "\n",
    "def handelstr(strs):\n",
    "  r=camel_case_split(strs)\n",
    "  r=replace_(r)\n",
    "  r=r.lower()\n",
    "  return r\n",
    "      \n",
    "# Driver code\n",
    "strs = \"GeeksForGeeks\"\n",
    "print(handelstr(strs).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a3b37e5a"
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('C:/Users/Fares_i9bkpvz/Desktop/data/webnlg-dataset-master/release_v2/json/webnlg_release_v2_train.json', encoding=\"utf8\") as f:\n",
    "    train = json.load(f)\n",
    "with open('C:/Users/Fares_i9bkpvz/Desktop/data/webnlg-dataset-master/release_v2/json/webnlg_release_v2_test.json', encoding=\"utf8\") as f:\n",
    "    test = json.load(f)\n",
    "with open('C:/Users/Fares_i9bkpvz/Desktop/data/webnlg-dataset-master/release_v2/json/webnlg_release_v2_dev.json', encoding=\"utf8\") as f:\n",
    "    dev = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dt7Ye6ZdcxrN"
   },
   "outputs": [],
   "source": [
    "def relexicalize(data,s,i):\n",
    "  e=s\n",
    "  category=handelstr(data['entries'][i][str(i+1)]['category'])\n",
    "  for j in range(len(data['entries'][i][str(i+1)]['modifiedtripleset'])):\n",
    "      \n",
    "      objectt=data['entries'][i][str(i+1)]['modifiedtripleset'][j]['object']\n",
    "      propertyy=handelstr(data['entries'][i][str(i+1)]['modifiedtripleset'][j]['property'])\n",
    "      subject=data['entries'][i][str(i+1)]['modifiedtripleset'][j]['subject']\n",
    "      e=e.replace(category.lower()+'k',subject)\n",
    "      e=e.replace(propertyy.lower()+'k',objectt)\n",
    "\n",
    "  return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63a9771c",
    "outputId": "eb26325a-1174-4fec-cb93-24fe19188b32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'389': {'category': 'Airport',\n",
       "  'lexicalisations': [{'comment': 'good',\n",
       "    'lex': 'Aarhus Airport, operated by Aarhus Lufthavn A/S, has a runway length of 2777.0.',\n",
       "    'xml_id': 'Id1'},\n",
       "   {'comment': 'good',\n",
       "    'lex': 'Aarhus Lufthavn A/S is the operation organisation of Aarhus Airport which has a runway length of 2777.',\n",
       "    'xml_id': 'Id2'}],\n",
       "  'modifiedtripleset': [{'object': '2777.0',\n",
       "    'property': 'runwayLength',\n",
       "    'subject': 'Aarhus_Airport'},\n",
       "   {'object': '\"Aarhus Lufthavn A/S\"',\n",
       "    'property': 'operatingOrganisation',\n",
       "    'subject': 'Aarhus_Airport'}],\n",
       "  'originaltriplesets': {'originaltripleset': [[{'object': '\"2777.0\"^^xsd:double',\n",
       "      'property': 'runwayLength',\n",
       "      'subject': 'Aarhus_Airport'},\n",
       "     {'object': '\"Aarhus Lufthavn A/S\"@en',\n",
       "      'property': 'operator',\n",
       "      'subject': 'Aarhus_Airport'}]]},\n",
       "  'shape': '(X (X) (X))',\n",
       "  'shape_type': 'sibling',\n",
       "  'size': '2',\n",
       "  'xml_id': 'Id1'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 387 test 1\n",
    "test['entries'][388]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "221c00ca"
   },
   "outputs": [],
   "source": [
    "def get_delexilized_data(data,k=-1):\n",
    "  y=[]\n",
    "  x=[]\n",
    "  if k==-1:\n",
    "    k=len(data['entries'])\n",
    "    print(k)\n",
    "  for i in range(k):\n",
    "      tar=data['entries'][i][str(i+1)]['lexicalisations'][0]['lex']\n",
    "      tar=handelstr(tar)\n",
    "      olist=[]\n",
    "      category=data['entries'][i][str(i+1)]['category']\n",
    "      category=handelstr(category)\n",
    "      for j in range(len(data['entries'][i][str(i+1)]['modifiedtripleset'])):\n",
    "          \n",
    "          objectt=handelstr(data['entries'][i][str(i+1)]['modifiedtripleset'][j]['object'])\n",
    "          propertyy=handelstr(data['entries'][i][str(i+1)]['modifiedtripleset'][j]['property'])\n",
    "          subject=handelstr(data['entries'][i][str(i+1)]['modifiedtripleset'][j]['subject'])\n",
    "          tar=tar.replace(subject,category.upper()+'k')\n",
    "          tar=tar.replace(objectt,propertyy.upper()+'k')\n",
    "          olist.append(category.upper()+'k')\n",
    "          olist.append(propertyy)\n",
    "          olist.append(propertyy.upper()+'k')\n",
    "      y.append(tar)    \n",
    "      x.append(olist)\n",
    "  return [x,y]\n",
    "\n",
    "\n",
    "def get_data(data,k=-1):\n",
    "  y=[]\n",
    "  x=[]\n",
    "  if k==-1:\n",
    "    k=len(data['entries'])\n",
    "    print(k)\n",
    "  for i in range(k):\n",
    "      y.append(data['entries'][i][str(i+1)]['lexicalisations'][0]['lex'])\n",
    "      olist=[]\n",
    "      for j in range(len(data['entries'][i][str(i+1)]['modifiedtripleset'])):       \n",
    "          objectt=data['entries'][i][str(i+1)]['modifiedtripleset'][j]['object']\n",
    "          propertyy=data['entries'][i][str(i+1)]['modifiedtripleset'][j]['property']\n",
    "          subject=data['entries'][i][str(i+1)]['modifiedtripleset'][j]['subject']\n",
    "          olist.append(objectt)\n",
    "          olist.append(propertyy)\n",
    "          olist.append(subject)\n",
    "      x.append(olist)\n",
    "  return [x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EU-8koVhtNIl",
    "outputId": "3de96567-d445-45a7-8eeb-ed4e2d946554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12876\n",
      "1600\n",
      "1619\n"
     ]
    }
   ],
   "source": [
    "x,y=get_delexilized_data(train)\n",
    "xtest,ytest=get_delexilized_data(test)\n",
    "xdev,ydev=get_delexilized_data(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87912c89",
    "outputId": "da1a173f-963b-41d1-c34b-82325d2ab33e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AIRPORTk', 'leader name', 'LEADER NAMEk']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdev[0]\n",
    "# train['entries'][909][str(909+1)]['lexicalisations'][0]['lex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "d8a8e14a",
    "outputId": "fca3eec7-6b17-4840-c6ab-7845c4a5b933"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the leader of AIRPORTk is LEADER NAMEk.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xp5tDLRGyUR3",
    "outputId": "62aef927-c13b-401b-b589-c2b4c59bc972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-05 16:15:01--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2022-01-05 16:15:02--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  4.95MB/s    in 2m 42s  \n",
      "\n",
      "2022-01-05 16:17:44 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: data/glove.6B.50d.txt   \n",
      "  inflating: data/glove.6B.100d.txt  \n",
      "  inflating: data/glove.6B.200d.txt  \n",
      "  inflating: data/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip 'glove.6B.zip' -d 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "27243f57",
    "outputId": "841ef5c6-2dff-46da-d237-556b6c8a7965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sostok the aarhus is the airport of aarhus, denmark. eostok'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add sostok and eostok at \n",
    "for i in range(len(y)):\n",
    "    y[i]='sostok '+ y[i] + ' eostok'\n",
    "    \n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6fa8e779"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='UNK')\n",
    "tokenizer.fit_on_texts(x)\n",
    "X_train1 = tokenizer.texts_to_sequences(x)\n",
    "xtest1 = tokenizer.texts_to_sequences(xtest)\n",
    "xdev1 = tokenizer.texts_to_sequences(xdev)\n",
    "\n",
    "tk = Tokenizer(oov_token='UNK')\n",
    "tk.fit_on_texts(y)\n",
    "y_train1 = tk.texts_to_sequences(y)\n",
    "ytest1 = tk.texts_to_sequences(ytest)\n",
    "ydev1 = tk.texts_to_sequences(ydev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "654713bd"
   },
   "outputs": [],
   "source": [
    "lengthey = []\n",
    "lengthex = []\n",
    "for i in range(len(X_train1)):\n",
    "    lengthex.append(len(X_train1[i]))\n",
    "\n",
    "for i in range(len(y_train1)):\n",
    "    lengthey.append(len(y_train1[i]))\n",
    "\n",
    "MAX_LENGTHy=max(lengthey)\n",
    "MAX_LENGTHx=max(lengthex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc155aa2",
    "outputId": "93989e72-72bb-472f-c009-2dddff6347f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 60, 61]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd255a32",
    "outputId": "d486345d-365d-4173-9e38-fda13714fd3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "vocab_sizex = len(tokenizer.word_index) + 1\n",
    "print(vocab_sizex)\n",
    "\n",
    "vocab_sizey = len(tk.word_index) + 1\n",
    "print(vocab_sizey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "10fccaac"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "y_train1 = pad_sequences(y_train1, maxlen=MAX_LENGTHy, padding='post')\n",
    "X_train1 = pad_sequences(X_train1, maxlen=MAX_LENGTHx, padding='post')\n",
    "xtest1 = pad_sequences(xtest1, maxlen=MAX_LENGTHx, padding='post')\n",
    "ytest1 = pad_sequences(ytest1, maxlen=MAX_LENGTHy, padding='post')\n",
    "xdev1 = pad_sequences(xdev1, maxlen=MAX_LENGTHx, padding='post')\n",
    "ydev1 = pad_sequences(ydev1, maxlen=MAX_LENGTHy, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff218519",
    "outputId": "dda2070c-05ea-4ec9-de4f-587f695cc13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12876\n",
      "12876\n",
      "sequence length:  21\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train1))\n",
    "print(len(y_train1))\n",
    "seq_length = X_train1.shape[1]\n",
    "print(\"sequence length: \", seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c341815f",
    "outputId": "1606d87a-44ed-4d00-feaf-f930727837bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 74)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "48875eef"
   },
   "outputs": [],
   "source": [
    "# 3115\n",
    "# 5512\n",
    "# 8174\n",
    "# 10667\n",
    "# 12518\n",
    "# 12706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DfRNkmmHyYzy"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix( word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "embedding_matrixx = create_embedding_matrix( tokenizer.word_index, embedding_dim)\n",
    "embedding_matrixy = create_embedding_matrix( tk.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_oH4o9ZMMxi"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ead39128",
    "outputId": "1e627f15-0360-44a3-f4c5-f89b2d3d467e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary from the w2v model = 762\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 21)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 21, 200)      152400      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 200)    1000200     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 21, 256),    467968      ['embedding[0][0]']              \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  467968      ['embedding_1[0][0]',            \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, None, 256)    0           ['lstm_1[0][0]',                 \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 512)    0           ['lstm_1[0][0]',                 \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 5001)  2565513     ['concatenate[0][0]']            \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,654,049\n",
      "Trainable params: 4,654,049\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed,Attention,Activation, dot,concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Size of vocabulary from the w2v model = {}\".format(vocab_sizex))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 256\n",
    "embedding_dim=200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(MAX_LENGTHx,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(vocab_sizex, embedding_dim,weights=[embedding_matrixx],trainable=True,mask_zero=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.3,recurrent_dropout=0.3)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(enc_emb)\n",
    "\n",
    "# Set up the decoder, using encoder_states as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(vocab_sizey, embedding_dim,weights=[embedding_matrixy],trainable=True,mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.3,recurrent_dropout=0.3)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# ATTENTION layer\n",
    " \n",
    "attn_layer = Attention()\n",
    "attn_out = attn_layer([decoder_outputs,encoder_outputs])\n",
    "decoder_concat_input = concatenate([decoder_outputs, attn_out])\n",
    "\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(vocab_sizey, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "772d9cc9"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop'\n",
    ", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1,patience=2)\n",
    "mc = ModelCheckpoint('best_model.hdf5', monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'max')\n",
    "# Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "# [MaskedCategoricalAccuracy(0),ExactMatchedAccuracy(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2439b3bc",
    "outputId": "bd00ecd4-9ace-40ce-9225-69cd6b81afcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 1.1072 - accuracy: 0.2407\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.29100, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 563s 3s/step - loss: 1.1072 - accuracy: 0.2407 - val_loss: 1.8458 - val_accuracy: 0.2910\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.7298 - accuracy: 0.4302\n",
      "Epoch 00002: val_accuracy improved from 0.29100 to 0.38551, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 550s 3s/step - loss: 0.7298 - accuracy: 0.4302 - val_loss: 1.4704 - val_accuracy: 0.3855\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.5071\n",
      "Epoch 00003: val_accuracy improved from 0.38551 to 0.43223, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 551s 3s/step - loss: 0.5894 - accuracy: 0.5071 - val_loss: 1.2974 - val_accuracy: 0.4322\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.5518\n",
      "Epoch 00004: val_accuracy improved from 0.43223 to 0.45993, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 550s 3s/step - loss: 0.5134 - accuracy: 0.5518 - val_loss: 1.1967 - val_accuracy: 0.4599\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.4649 - accuracy: 0.5821\n",
      "Epoch 00005: val_accuracy improved from 0.45993 to 0.48597, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 549s 3s/step - loss: 0.4649 - accuracy: 0.5821 - val_loss: 1.1278 - val_accuracy: 0.4860\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.6046\n",
      "Epoch 00006: val_accuracy improved from 0.48597 to 0.50190, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 548s 3s/step - loss: 0.4281 - accuracy: 0.6046 - val_loss: 1.0743 - val_accuracy: 0.5019\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.6236\n",
      "Epoch 00007: val_accuracy improved from 0.50190 to 0.51063, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 547s 3s/step - loss: 0.4000 - accuracy: 0.6236 - val_loss: 1.0414 - val_accuracy: 0.5106\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.6391\n",
      "Epoch 00008: val_accuracy improved from 0.51063 to 0.52343, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 547s 3s/step - loss: 0.3771 - accuracy: 0.6391 - val_loss: 1.0173 - val_accuracy: 0.5234\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.3592 - accuracy: 0.6520\n",
      "Epoch 00009: val_accuracy improved from 0.52343 to 0.53182, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 548s 3s/step - loss: 0.3592 - accuracy: 0.6520 - val_loss: 0.9970 - val_accuracy: 0.5318\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.6631\n",
      "Epoch 00010: val_accuracy improved from 0.53182 to 0.53832, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 549s 3s/step - loss: 0.3424 - accuracy: 0.6631 - val_loss: 0.9810 - val_accuracy: 0.5383\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.6720\n",
      "Epoch 00011: val_accuracy improved from 0.53832 to 0.54435, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 548s 3s/step - loss: 0.3288 - accuracy: 0.6720 - val_loss: 0.9683 - val_accuracy: 0.5444\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.6823\n",
      "Epoch 00012: val_accuracy improved from 0.54435 to 0.54966, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 546s 3s/step - loss: 0.3165 - accuracy: 0.6823 - val_loss: 0.9524 - val_accuracy: 0.5497\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.6897\n",
      "Epoch 00013: val_accuracy improved from 0.54966 to 0.55345, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 545s 3s/step - loss: 0.3052 - accuracy: 0.6897 - val_loss: 0.9448 - val_accuracy: 0.5534\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.6968\n",
      "Epoch 00014: val_accuracy improved from 0.55345 to 0.56030, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 546s 3s/step - loss: 0.2950 - accuracy: 0.6968 - val_loss: 0.9346 - val_accuracy: 0.5603\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.2859 - accuracy: 0.7043\n",
      "Epoch 00015: val_accuracy improved from 0.56030 to 0.56064, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 549s 3s/step - loss: 0.2859 - accuracy: 0.7043 - val_loss: 0.9278 - val_accuracy: 0.5606\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.7104\n",
      "Epoch 00016: val_accuracy improved from 0.56064 to 0.56141, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 545s 3s/step - loss: 0.2774 - accuracy: 0.7104 - val_loss: 0.9256 - val_accuracy: 0.5614\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.7151\n",
      "Epoch 00017: val_accuracy improved from 0.56141 to 0.56505, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 543s 3s/step - loss: 0.2699 - accuracy: 0.7151 - val_loss: 0.9179 - val_accuracy: 0.5650\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.7194\n",
      "Epoch 00018: val_accuracy improved from 0.56505 to 0.56683, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 546s 3s/step - loss: 0.2629 - accuracy: 0.7194 - val_loss: 0.9174 - val_accuracy: 0.5668\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.7255\n",
      "Epoch 00019: val_accuracy improved from 0.56683 to 0.56864, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 546s 3s/step - loss: 0.2564 - accuracy: 0.7255 - val_loss: 0.9164 - val_accuracy: 0.5686\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.7298\n",
      "Epoch 00020: val_accuracy improved from 0.56864 to 0.57047, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 545s 3s/step - loss: 0.2500 - accuracy: 0.7298 - val_loss: 0.9118 - val_accuracy: 0.5705\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.7350\n",
      "Epoch 00021: val_accuracy did not improve from 0.57047\n",
      "161/161 [==============================] - 547s 3s/step - loss: 0.2438 - accuracy: 0.7350 - val_loss: 0.9174 - val_accuracy: 0.5677\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.7392\n",
      "Epoch 00022: val_accuracy improved from 0.57047 to 0.57176, saving model to best_model.hdf5\n",
      "161/161 [==============================] - 544s 3s/step - loss: 0.2385 - accuracy: 0.7392 - val_loss: 0.9105 - val_accuracy: 0.5718\n",
      "Epoch 23/50\n",
      " 26/161 [===>..........................] - ETA: 7:00 - loss: 0.2240 - accuracy: 0.7530"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-438-7211752835a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                              workers=3, use_multiprocessing=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=model.fit([X_train1,y_train1[:,:-1]], y_train1.reshape(y_train1.shape[0],y_train1.shape[1], 1)[:,1:] ,\n",
    "                  epochs=50,callbacks=[es,mc]\n",
    "                  ,batch_size=64, validation_split=0.2,shuffle=True,\n",
    "                             workers=3, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "08a8c1ec"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=tk.index_word\n",
    "reverse_source_word_index=tokenizer.index_word\n",
    "target_word_index=tk.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa68iIrZxoih"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6f19551c"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(MAX_LENGTHx,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf = attn_layer([ decoder_outputs2,decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "73aeee7a"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if(sampled_token_index==0):\n",
    "            break\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (MAX_LENGTHy-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4c3c3e01"
   },
   "outputs": [],
   "source": [
    "def seq2target(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "126b46d0",
    "outputId": "346e65dc-c46f-431d-8f10-190c564c3b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Allen_Forrest birth year 1981 Allen_Forrest birth place \"Fort Campbell, KY, raised in Dothan, AL\" Allen_Forrest background \"solo_singer\" Allen_Forrest genre Pop_music Allen_Forrest birth place \"Fort Campbell, KY, raised in Dothan, AL\" \n",
      "Original summary: Allen_Forrest is a solo Pop_music singer who was born in 1981 in \"Fort Campbell, KY, raised in Dothan, AL\" kentucky but was raised in dothan alabama \n",
      "Predicted summary:  Allen_Forrest was born in 1981 in \"Fort Campbell, KY, raised in Dothan, AL\" he plays Pop_music and he plays Pop_music music and is a solo singer\n",
      "\n",
      "\n",
      "Review: Allen_Forrest birth year 1981 Allen_Forrest birth place Dothan,_Alabama Allen_Forrest genre Pop_music Allen_Forrest birth place Dothan,_Alabama Allen_Forrest background \"solo_singer\" \n",
      "Original summary: Allen_Forrest is an exponent of Pop_music and a solo singer who was born in the year 1981 in dothan Dothan,_Alabama alabama \n",
      "Predicted summary:  solo singer Allen_Forrest was born in Dothan,_Alabama in 1981 he plays Pop_music and is a solo singer\n",
      "\n",
      "\n",
      "Review: Allen_Forrest birth year 1981 Allen_Forrest genre Acoustic_music Allen_Forrest birth place \"Fort Campbell, KY, raised in Dothan, AL\" Allen_Forrest background \"solo_singer\" Allen_Forrest birth place \"Fort Campbell, KY, raised in Dothan, AL\" \n",
      "Original summary: Allen_Forrest was born in the year 1981 in \"Fort Campbell, KY, raised in Dothan, AL\" ky and was raised in dothan al he was originally a solo singer and performs Acoustic_music \n",
      "Predicted summary:  Acoustic_music singer Allen_Forrest was born in \"Fort Campbell, KY, raised in Dothan, AL\" in 1981 he is a solo singer and his musical genre is Acoustic_music\n",
      "\n",
      "\n",
      "Review: Allen_Forrest genre Hip_hop_music Allen_Forrest birth year 1981 Allen_Forrest birth place \"Fort Campbell, KY, raised in Dothan, AL\" Allen_Forrest background \"solo_singer\" Allen_Forrest birth place \"Fort Campbell, KY, raised in Dothan, AL\" \n",
      "Original summary: Allen_Forrest is a Hip_hop_music artist and solo singer who was born in the year 1981 in \"Fort Campbell, KY, raised in Dothan, AL\" ky and was raised in dothan al \n",
      "Predicted summary:  Hip_hop_music singer Allen_Forrest was born in \"Fort Campbell, KY, raised in Dothan, AL\" in 1981 he is a solo singer and performs Hip_hop_music music\n",
      "\n",
      "\n",
      "Review: Allen_Forrest genre Hip_hop_music Allen_Forrest birth year 1981 Allen_Forrest birth place Dothan,_Alabama Allen_Forrest background \"solo_singer\" Allen_Forrest birth place Dothan,_Alabama \n",
      "Original summary: Allen_Forrest was a solo singer and hip hop artist born in 1981 in dothan Dothan,_Alabama alabama \n",
      "Predicted summary:  Hip_hop_music singer Allen_Forrest was born in Dothan,_Alabama in 1981 he is a solo singer and performs Hip_hop_music music\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(11000,11005):\n",
    "    print(\"Review:\",relexicalize(train,seq2text(X_train1[i]),i))\n",
    "    print(\"Original summary:\",relexicalize(train,seq2target(y_train1[i]),i))\n",
    "    print(\"Predicted summary:\",relexicalize(train,decode_sequence(X_train1[i].reshape(1,MAX_LENGTHx)),i))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOxj2D4nvFjM",
    "outputId": "f1fb446e-aeb2-41df-9d63-a29abd05cd3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Bhajji country India Bhajji currency Indian_rupee Bhajji leader name Narendra_Modi Bhajji leader name Narendra_Modi \n",
      "Original summary: Bhajji originates from India where the currency is the Indian rupee and the leaders are Narendra Modi and Sumitra Mahajan.\n",
      "Predicted summary:  Bhajji is from India where the leader is the Indian rupee and the leader is Narendra_Modi and the currency is the Indian rupee\n",
      "\n",
      "\n",
      "Review: Bhajji region Karnataka Bhajji main ingredients \"Gram flour, vegetables\" Bhajji alternative name \"Bhaji, bajji\" Bhajji ingredient Gram_flour \n",
      "Original summary: Bhajji, otherwise known as Bhaji or bajji, comes from the Karnataka region. The main ingredients are gram flour and vegetables.\n",
      "Predicted summary:  Bhajji is from the Karnataka and main ingredients include red beans pork belly white rice ground meat chicharon fried egg plantain patacones chorizo arepa hogao sauce black pudding morcilla avocado and lemon\n",
      "\n",
      "\n",
      "Review: Binignit ingredient Sweet_potato Binignit course Dessert Binignit main ingredients Banana Binignit dish variation Sandesh_(confectionery) \n",
      "Original summary: Binignit is a type of dessert, like sandesh, that contains sweet potato and banana.\n",
      "Predicted summary:  Binignit is a Dessert as made from the Dessert course it is also a Dessert Sandesh_(confectionery) is also a Dessert\n",
      "\n",
      "\n",
      "Review: Binignit main ingredients Sweet_potato Binignit ingredient Banana Binignit course Dessert Binignit dish variation Sandesh_(confectionery) \n",
      "Original summary: Binignit is a dessert. Its main ingredient is sweet potato and it also contains banana. Sandesh (confectionery) can also be served as dessert.\n",
      "Predicted summary:  Binignit is a Dessert as Dessert as its main ingredient is Banana sandesh Banana\n",
      "\n",
      "\n",
      "Review: Bionico country Mexico Bionico course Dessert Bionico region Guadalajara Bionico ingredient Granola \n",
      "Original summary: Bionico is from the Guadalajara region of Mexico. It is a dessert that includes granola.\n",
      "Predicted summary:  Bionico is a dish from Guadalajara Mexico it is also found in Mexico and includes Granola as one of its ingredients\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1200,1205):\n",
    "    print(\"Review:\",relexicalize(test,seq2text(xtest1[i]),i))\n",
    "    print(\"Original summary:\",test['entries'][i][str(i+1)]['lexicalisations'][0]['lex'])\n",
    "    print(\"Predicted summary:\",relexicalize(test,decode_sequence(xtest1[i].reshape(1,MAX_LENGTHx)),i))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0Fw9bid803G",
    "outputId": "063ad278-7901-4b6e-c499-80d84553430e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Aarhus leader name Jacob_Bundsgaard \n",
      "Original summary: The leader of Aarhus is Jacob Bundsgaard.\n",
      "Predicted summary:  Aarhus's leader name is Jacob_Bundsgaard\n",
      "\n",
      "\n",
      "Review: Aarhus_Airport runway length 2702.0 \n",
      "Original summary: Aarhus Airport's runway length is 2702.0.\n",
      "Predicted summary:  the runway length of Aarhus_Airport is 2702.0\n",
      "\n",
      "\n",
      "Review: Adirondack_Regional_Airport elevation above the sea level (in metres) 507 \n",
      "Original summary: Adirondack Regional Airport is 507 metres above sea level.\n",
      "Predicted summary:  Adirondack_Regional_Airport is elevation above the sea level in metres k metres above sea level\n",
      "\n",
      "\n",
      "Review: Adirondack_Regional_Airport location Harrietstown,_New_York \n",
      "Original summary: Adirondack Regional airport is located at Harrietstown, New York.\n",
      "Predicted summary:  Adirondack_Regional_Airport is located in Harrietstown,_New_York\n",
      "\n",
      "\n",
      "Review: Adolfo_Suárez_Madrid–Barajas_Airport location San_Sebastián_de_los_Reyes \n",
      "Original summary: Adolfo Suárez Madrid–Barajas Airport is found in San Sebastián de los Reyes.\n",
      "Predicted summary:  Adolfo_Suárez_Madrid–Barajas_Airport is located in San_Sebastián_de_los_Reyes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(\"Review:\",relexicalize(dev,seq2text(xdev1[i]),i))\n",
    "    print(\"Original summary:\",dev['entries'][i][str(i+1)]['lexicalisations'][0]['lex'])\n",
    "    print(\"Predicted summary:\",relexicalize(dev,decode_sequence(xdev1[i].reshape(1,MAX_LENGTHx)),i))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def BLEU(data,input,k=-1):\n",
    " count=0\n",
    "  if k==-1:\n",
    "    k=len(data['entries'])\n",
    "  for i in range(k):\n",
    "    ref=[]\n",
    "    for j in range(len(data['entries'][i][str(i+1)]['lexicalisations'])):\n",
    "      tar=data['entries'][i][str(i+1)]['lexicalisations'][j]['lex']\n",
    "      tar=tar.split()\n",
    "      ref.append(tar)\n",
    "    pridiction = relexicalize(data,decode_sequence(input[i].reshape(1,MAX_LENGTHx)),i).split()\n",
    "    count=count+sentence_bleu(ref, pridiction)\n",
    "  return count/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "GkOnGm2CC9pX"
   },
   "outputs": [],
   "source": [
    "c2=BLEU(test,xtest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "DKRbr1a-DQ45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1228186356312705"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "qjooBx60EPcM"
   },
   "outputs": [],
   "source": [
    "c3=BLEU(dev,xdev1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "DqwgQ1GsFnY_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11930895862149012"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "IAZX71b1Fn66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1328589755694775"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1=BLEU(train,X_train1)\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hx4Bcu9EAcqd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
